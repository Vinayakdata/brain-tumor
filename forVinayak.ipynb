{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\kiit\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (9, 9), input_shape=(64, 64, 3..., activation=\"relu\")`\n",
      "  from ipykernel import kernelapp as app\n",
      "c:\\users\\kiit\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (6, 6), input_shape=(64, 64, 3..., activation=\"relu\")`\n",
      "c:\\users\\kiit\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:21: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), input_shape=(64, 64, 3..., activation=\"relu\")`\n",
      "c:\\users\\kiit\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:24: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), input_shape=(64, 64, 3..., activation=\"relu\")`\n"
     ]
    }
   ],
   "source": [
    "#13 model\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense \n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D \n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "\n",
    "classifier = Sequential()\n",
    "\n",
    "classifier.add(Convolution2D(64, 9, 9, input_shape = (64, 64, 3), activation='relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "classifier.add(Convolution2D(32, 6, 6, input_shape = (64, 64, 3), activation='relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "classifier.add(Convolution2D(32, 3, 3, input_shape = (64, 64, 3), activation='relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "classifier.add(Convolution2D(16, 3, 3, input_shape = (64, 64, 3), activation='relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "\n",
    "\n",
    "classifier.add(Flatten())\n",
    "classifier.add(Dense(units = 128, activation = 'relu' ))\n",
    "classifier.add(Dense(units = 2, activation = 'softmax'))\n",
    "\n",
    "classifier.compile(optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 56, 56, 64)        15616     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 23, 23, 32)        73760     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 11, 11, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 9, 9, 32)          9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 2, 2, 16)          4624      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 1, 1, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               2176      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 105,682\n",
      "Trainable params: 105,682\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "from keras.preprocessing.image import ImageDataGenerator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 202 images belonging to 2 classes.\n",
      "Found 51 images belonging to 2 classes.\n",
      "Epoch 1/10\n",
      "20/20 [==============================] - 22s 1s/step - loss: 0.6618 - accuracy: 0.6150 - val_loss: 0.6850 - val_accuracy: 0.6076\n",
      "Epoch 2/10\n",
      "20/20 [==============================] - 21s 1s/step - loss: 0.6352 - accuracy: 0.6185 - val_loss: 0.6465 - val_accuracy: 0.6081\n",
      "Epoch 3/10\n",
      "20/20 [==============================] - 21s 1s/step - loss: 0.6431 - accuracy: 0.6045 - val_loss: 0.6232 - val_accuracy: 0.6086\n",
      "Epoch 4/10\n",
      "20/20 [==============================] - 21s 1s/step - loss: 0.6334 - accuracy: 0.6254 - val_loss: 0.6615 - val_accuracy: 0.6469\n",
      "Epoch 5/10\n",
      "20/20 [==============================] - 21s 1s/step - loss: 0.6511 - accuracy: 0.6326 - val_loss: 0.6174 - val_accuracy: 0.6081\n",
      "Epoch 6/10\n",
      "20/20 [==============================] - 21s 1s/step - loss: 0.6089 - accuracy: 0.6202 - val_loss: 0.5792 - val_accuracy: 0.8028\n",
      "Epoch 7/10\n",
      "20/20 [==============================] - 21s 1s/step - loss: 0.6113 - accuracy: 0.6603 - val_loss: 0.5567 - val_accuracy: 0.6081\n",
      "Epoch 8/10\n",
      "20/20 [==============================] - 21s 1s/step - loss: 0.5805 - accuracy: 0.6544 - val_loss: 0.4024 - val_accuracy: 0.6076\n",
      "Epoch 9/10\n",
      "20/20 [==============================] - 21s 1s/step - loss: 0.5493 - accuracy: 0.7138 - val_loss: 0.5871 - val_accuracy: 0.6076\n",
      "Epoch 10/10\n",
      "20/20 [==============================] - 21s 1s/step - loss: 0.5138 - accuracy: 0.7300 - val_loss: 0.4159 - val_accuracy: 0.9208\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x27cc17de358>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale = 1./255, shear_range = .2, zoom_range = .2, horizontal_flip = True)\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory('dataset/training_set', target_size = (64, 64), batch_size = 32, class_mode = 'categorical')\n",
    "test_set = test_datagen.flow_from_directory('dataset/test_set', target_size = (64, 64), batch_size = 32, class_mode = 'categorical')\n",
    "\n",
    "classifier.fit_generator(training_set, steps_per_epoch = int(20+31/32), epochs = 10, validation_data = test_set, validation_steps = int(78+124/32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "normal_cases_dir = r'dataset/test_set/no'\n",
    "idc_cases_dir = r'dataset/test_set/yes'\n",
    "\n",
    "normal_cases = glob.glob(normal_cases_dir+'/*.*') #change to .png or .jpg if images are in that format\n",
    "idc_cases = glob.glob(idc_cases_dir+'/*.*')\n",
    "'''normal_cases1 = glob.glob(normal_cases_dir+'/*.JPG')\n",
    "idc_cases1 = glob.glob(idc_cases_dir+'/*.JPG')\n",
    "normal_cases2 = glob.glob(normal_cases_dir+'/*.jpeg')\n",
    "idc_cases2 = glob.glob(idc_cases_dir+'/*.jpeg')\n",
    "\n",
    "\n",
    "normal_cases.extend(normal_cases1)\n",
    "idc_cases.extend(idc_cases1)\n",
    "\n",
    "normal_cases.extend(normal_cases2)\n",
    "idc_cases.extend(idc_cases2)'''\n",
    "\n",
    "testing_data = []\n",
    "testing_labels = []\n",
    "import cv2\n",
    "import keras\n",
    "\n",
    "for img in normal_cases:\n",
    "    img = cv2.imread(str(img))\n",
    "    img = cv2.resize(img, (64,64)) #HERE, CHANGE (50,50) TO WHATEVER IMAGE INPUT SIZE YOU HAVE TAKEN\n",
    "    if img.shape[2] ==1:\n",
    "        img = np.dstack([img, img, img])\n",
    "    else:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = img.astype(np.float32)/255.\n",
    "    label = keras.utils.to_categorical(0, num_classes=2)\n",
    "    testing_data.append(img)\n",
    "    testing_labels.append(label)\n",
    "                      \n",
    "for img in idc_cases:\n",
    "    img = cv2.imread(str(img))\n",
    "    img = cv2.resize(img, (64,64)) #CHANGE HERE AGAIN\n",
    "    if img.shape[2] ==1:\n",
    "        img = np.dstack([img, img, img])\n",
    "    else:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = img.astype(np.float32)/255.\n",
    "    label = keras.utils.to_categorical(1, num_classes=2)\n",
    "    testing_data.append(img)\n",
    "    testing_labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of test examples:  (51, 64, 64, 3)\n",
      "Total number of labels: (51, 2)\n"
     ]
    }
   ],
   "source": [
    "testing_data = np.array(testing_data)\n",
    "testing_labels = np.array(testing_labels)\n",
    "\n",
    "print(\"Total number of test examples: \", testing_data.shape)\n",
    "print(\"Total number of labels:\", testing_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51/51 [==============================] - 0s 3ms/step\n",
      "Loss on test set:  0.4326406045287263\n",
      "Accuracy on test set:  0.9215686321258545\n",
      "(51,)\n",
      "(51,)\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_score = classifier.evaluate(testing_data, testing_labels, batch_size=32)\n",
    "print(\"Loss on test set: \", test_loss)\n",
    "print(\"Accuracy on test set: \", test_score)\n",
    "\n",
    "\n",
    "preds = classifier.predict(testing_data, batch_size = 32)\n",
    "preds = np.argmax(preds, axis=-1)\n",
    "\n",
    "orig_test_labels = np.argmax(testing_labels, axis=-1)\n",
    "print(orig_test_labels.shape)\n",
    "print(preds.shape)\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(orig_test_labels, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[19,  1],\n",
       "       [ 3, 28]], dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "sn.set(font_scale=1.4) # for label size\n",
    "sn.heatmap(cm, annot=True, annot_kws={\"size\": 16})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
